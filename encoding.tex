\section{SMT Encoding}
The SMT encoding is generated from 1) an execution trace of an MPI program that includes a sequence of events, i.e., point-to-point operations, collective operations, control flow assumptions and property assertions; and 2) a set of possible match pairs. Intuitively, a match pair is a coupling of a send and a receive where this send may match this receive in runtime. Taking the message communication in \figref{fig:mpi} as an example, each receive on process $0$ may be matched with a send on process $1$ or a send on process $2$. The direct use of match pair, rather than a state based approach \cite{} or a indirect use of match pair in an order based encoding \cite{}, is easy to reason about the message non-determinism. 

Given a new encoding that constrains an MPI execution, a modern SMT solver, such as Yice \cite{} or Z3 \cite{}, is able to search for a satisfying assignment to a set of variables defined in the encoding. As the potential send-receive matches are encoded, the SMT solver is able to resolve the message non-determinism in such a way that each send (receive) is assigned a receive (send). Such a resolution of message communication is also correct in the system runtime. If a satisfying assignment exists, the total order of all events resolved by the solver represents a schedule that follows the assumed control flow path and message communication but violates the assertion (the assertion is negated in the encoding). If a satisfying assignment is not found, any trace by executing those events is proved not to violate the assertion (the assumed control flow path is still followed).

%: assume infinite buffer in definition
This algorithm is also affected by two semantics: infinite-buffering and zero-buffering. As discussed earlier, the infinite-buffering behaves differently compared to the zero-buffering in how message communicates. 
%This behavior difference also affects the way to encode them. 
This section first discusses the algorithm to encode infinite-buffering, including the rules for send, receive, barrier and how they are constrained in program order. It then discusses how to adjust infinite-buffering encoding to zero-buffering semantics by adding some new rules.

\subsection{Partial Order}
The encoding needs to express the partial order imposed by MPI semantics as SMT constraints. This partial order encoding needs to bind an ``order" variable (\defref{def:order}) to each event in the program. The pertinent ``orders" of two events are used to constrain their partial order as a \emph{happens-before} relation (\defref{def:happens-before}).

\begin{definition}[Order]\label{def:order}
The order of an event $\mathtt{e}$, denoted as $\mathit{order}_\mathtt{e}$, is constrained to an integer.
\end{definition}

\begin{definition}[Happens-Before]\label{def:happens-before}
The \emph{Happens-Before} $(\mathtt{HB})$ relation over two event orders $\mathit{order}_\mathtt{e1}$ and $\mathit{order}_\mathtt{e2}$, denoted as
$\mathit{order}_\mathtt{e1} \mathrm{\prec_\mathtt{HB}} \mathit{order}_\mathtt{e2}$, is a partial order constraint with a logic formula $\mathit{order}_\mathtt{e1} <  \mathit{order}_\mathtt{e2}$. 
\label{def:hb}
\end{definition}

In \defref{def:happens-before}, if $\mathtt{e1}$ has to be issued before $\mathtt{e2}$, $\mathit{order}_\mathtt{e1} \mathrm{\prec_\mathtt{HB}} \mathit{order}_\mathtt{e2}$ is constrained in the encoding. Consequently, it asks an SMT solver to search for a feasible valuation for each of the ``orders" to make this constraint true. Based on the partial order encoding, the algorithm further defines a set of program order as SMT constraints (discussed later in \emph{Program Order}). Those constraints include the sequential behavior in each process model and the concurrent behavior among processes. 

\subsection{Point-To-Point Communication}
%:Definitions
Before discussing the program order, it is necessary to take a look at the point-to-point operations as SMT constraints. As an essential part of MPI semantics, the asynchronous message passing consists of two basic communication primitives: send and receive. The encoding needs to define send (\defref{def:snd}) and receive (\defref{def:rcv}) as a set of variables in order to constrain the concurrent behavior. The evaluation of some variables, such as the endpoint in a send or a receive, are statically determined since the event is from an existing trace. Other variables, such as the event order, need to be determined by running an SMT solver. 
 
\begin{definition}[Send] \label{def:snd}
A send operation $\mathtt{S}$, is a four-tuple of variables:
\begin{compactenum}
\item $M_\mathtt{S}$, the order of the matching receive event;
\item $\mathit{order}_\mathtt{S}$, the order of the send;
\item $e_\mathtt{S}$, the endpoint; 
\item $src_\mathtt{S}$, the source endpoint; and
\item $\mathit{value}_\mathtt{S}$, the transmitted value.
\end{compactenum}
\end{definition}

Other than the static data (endpoints and transmitted value) that constrains the basic information of a send, the event order $\mathit{order}_\mathtt{S}$ and the event order of its matching receive $M_\mathtt{S}$ are encoded.  As mentioned above, a suitable assignment to this variable is only determined when an SMT solver tries to resolve it. As a message communication primitive, the ``order" of the matching receive is used to bind a specific receive if a message flows from $\mathtt{S}$ to this receive in a resolved execution. Again, the assignment is only determined when running an SMT solver. Once the value is assigned, the constraint does not allow $\mathtt{S}$ to match with another receive (otherwise it is a false constraint).

In MPI semantics, a non-blocking send is paired with a wait operation, which returns when data has been copied out of the send buffer. Issuing a wait operation may or may not affect the message communication, depending on which semantics in runtime. As for infinite buffering, the wait operation does not have a control on the match pair decision because it only confirms the data has been transferred to the buffer. The message can be delivered after the wait operation returns. As such, the algorithm does not encode the order over a send and its wait for infinite buffering. In contrast, the zero buffering encoding is different due to the semantics. The difference and the revision on encoding rules are discussed later in \textit{Zero-Buffering Encoding}. 
 
\begin{definition}[Wait] \label{def:wait}
The occurrence of a wait operation, \texttt{W}, is captured by a
single variable, $\mathit{order}_\mathtt{W}$, that constrains when
the wait occurs.
\end{definition}

A wait can also be paired with a non-blocking receive but is able to confirm the message delivery this time. This is because the wait may not return until the receive is issued and the message is copied into the receive buffer. In other words, a necessary condition for returning the wait operation is a message has arrived at the receive. The behavior confirms two things: 1) the receive is issued before the wait; and 2) the matching send is issued before the wait. Consequently, this triggers the encoding rules to constrain such a behavior. Again, the rules are discussed later in \textit{Program Order}.  Further, the MPI semantics allow a single wait to witness the completion of many receives due to the message non-overtaking property. A wait that witnesses the completion of one or more receives is the nearest-enclosing wait.

\begin{definition}[Nearest-Enclosing Wait] \label{def:nw}
A wait that witnesses the completion of a receive by indicating that
the message is delivered and that all the previous receives in the
same task issued earlier are complete as well.
\end{definition}
%: an example to show nearest-encolsing wait

The encoding requires that every receive has a nearest-enclosing wait as it makes the match pair decisions at the wait. Based on this requirement, a receive can be defined as a set of variables in \defref{def:rcv}.

\begin{definition}[Receive] \label{def:rcv}
A receive operation $\mathtt{R}$ is modeled by a five-tuple of variables:
\begin{compactenum}
\item $M_\mathtt{R}$, the order of the matching send event;
\item $\mathit{order}_\mathtt{R}$, the order of the receive;
\item $e_\mathtt{R}$, the endpoint;
\item $src_\mathtt{R}$, the source endpoint;
\item $\mathit{value}_\mathtt{R}$, the received value; and,
\item $\mathit{nw}_\mathtt{R}$, the order of the nearest enclosing wait.
\end{compactenum}
\end{definition}

Similarly, a receive has its endpoint statically known. It also defines two free variables for the event order and the order of the matching send, respectively. The transmitted (received) value is not static now because it is not known until a matching send is determined. Interestingly, the source endpoint for a receive may or may not be static because the MPI semantics support both deterministic receive (a constant source endpoint) and wildcard receive (a uncertain endpoint). As for a deterministic receive, the source endpoint is a static data. In contrast, it is only determined in match pair decisions for a wildcard receive. The order of the nearest-enclosing wait, as discussed before, is used to make match pair decisions. It is statically evaluated from the existing execution.

Given point-to-point operations, the algorithm expresses a message communication by explicitly encoding a send-receive match as a set of constraints in \defref{def:match}. Informally, \defref{def:match} constrains the event order, the endpoints and the transmitted value are matched for $\mathtt{R}$ and $\mathtt{S}$. Further, $\mathtt{S}$ should be issued before the nearest-enclosing wait of $\mathtt{R}$. 

\begin{definition}[Match Pair] \label{def:match}
A match pair, $\langle\mathtt{R}, \mathtt{S}\rangle$, for a receive
$\mathtt{R}$ and a send $\mathtt{S}$ corresponds to the constraints:
\begin{compactenum}
\item $M_{\mathtt{R}} = \mathit{order}_{\mathtt{S}}$
\item $M_{\mathtt{S}} = \mathit{order}_{\mathtt{R}}$
\item $e_{\mathtt{R}} = e_{\mathtt{S}}$
\item $src_\mathtt{R} = src_\mathtt{S}$
\item $\mathit{value}_{\mathtt{R}} = \mathit{value}_{\mathtt{S}}$ and
\item $\mathit{order}_{\mathtt{S}}\ \mathrm{\prec_\mathtt{HB}}\ \mathit{nw}_{\mathtt{R}}$
\end{compactenum}
\end{definition}

\begin{definition}[Receive Matches] \label{def:receive_match}
For each receive $\mathtt{R}$, if $\langle\mathtt{R},
\mathtt{S}_0\rangle$ through $\langle\mathtt{R}, \mathtt{S}_n\rangle$
are match pairs, then $\bigvee_{i}^{n} \langle\mathtt{R},
\mathtt{S}_i\rangle$ is used as an SMT constraint.
\end{definition}

The encoding is given a set of potential set of match pairs over all sends and receives in the existing execution trace. Since those match pairs reflect all possible ways the messages may flow across all receives, there may exist several match pairs for a single receive. However, there is no way to satisfy all match pairs for a single receive at a time, even though all possible choices are considered. Therefore, the encoding does not combine all match pairs in a single conjunction. Instead, it constrains all match pairs for a single receive in a disjunction (\defref{def:receive_match}). As the $M$ values for both a receive and a send are determined once they are matched, only one choice for a single receive and a single send can be used in a final resolution.

\subsection{Collective Communication}
As another significant part of MPI semantics, the collective operations synchronize a program in such a way that each process operates a task by evaluating the execution of all other processes. This paper only considers barrier in the discussion as it is the most interesting and the most frequently used collective operation. In MPI semantics, each process waits on a barrier without proceeding until all barriers are witnessed. Therefore, any operation issued before the barriers can not interleave any operation issued after the barriers, and vice versa. The encoding defines the barrier in \defref{def:barrier}. It can be used to define the program order caused by barriers.

\begin{definition}[Barrier]\label{def:barrier}
The occurrence of a barrier operation, \texttt{B}, is captured by a
single variable, $\mathit{order}_\mathtt{B}$, that constrains when a group of barriers $\{B_0, B_1, ..., B_n\}$ are matched.  
Each barrier $B_i, i\in{0 ... n}$, is issued by process $i$. 
\end{definition}

\begin{figure}[h]
\[
\begin{array}{l|l}
\;\;\;\;\;\;\;\;\mathtt{Process\ 0}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{Process\ 1}\;\;\;\;\;\;\;\; \\
\hline
\;\;\;\;\;\;\;\;\mathtt{\underline{B(comm)}}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{R(from\ P0,A\&h2)}\;\;\;\;\;\;\;\; \\
\;\;\;\;\;\;\;\;\mathtt{S(to\ P1,``1",\&h1)}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{\underline{B(comm)}}\;\;\;\;\;\;\;\; \\
\;\;\;\;\;\;\;\;\mathtt{W(\&h1)}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{W(\&h2)}\;\;\;\;\;\;\;\; \\
\end{array}
\]
\caption{An Example of Message Communication with Barriers} \label{fig:mc_barrier1}
\end{figure}

Unlike process interleaving, the message communication may not be affected by barriers. As an example, an execution of \figref{fig:mc_barrier1} may flow the message ``$1$" across $\mathtt{R}$ even though $\mathtt{R}$ is issued before the barriers and $\mathtt{S}$ is issued after the barriers. However, if the program issues $\mathtt{W(\&h2)}$ before barriers, $\mathtt{R}$ has to be completed meaning a message must be copied into the receive buffer. In this situation, the barriers affect the message communication. Such an example is in \figref{fig:mpi_barrier}. Therefore, the message communication has to be constrained based on the receives, the waits and the barriers. The \textit{nearest-enclosing barrier} (\defref{def:nb}) is helpful in such a constraint.

\begin{definition}[Nearest-Enclosing Barrier]\label{def:nb}
For a process $i$, a receive \texttt{R} has a nearest-enclosing barrier \texttt{B} if and only if
\begin{compactenum}
\item the nearest-enclosing wait \texttt{W} of \texttt{R} is issued before $B_i\in B$, and
\item there does not exist any receive \texttt{R'} that has its nearest-enclosing wait \texttt{W'} issued after \texttt{W} and issued before $B_i$.
\end{compactenum}
\end{definition}


\subsection{Assumptions and Assertions}
The existence of an assumption constrains a feasible execution to obey the imposed control flow path. Therefore, It should be viewed as a inviolated truth in any execution. As such, it is encoded as an assertion. 
\begin{definition}[Assumption] \label{def:assm}
Every assumption $\mathtt{A}$ is added as an SMT assertion.
\end{definition}

Another truth that any execution should obey is the property assertion. The goal of an SMT solver is to find a satisfying assignment that violates the property. In other words, the SMT solver tries to find the \textit{negation} of this property. Therefore, the property assertion should be negated in the encoding.
\begin{definition}[Property Assertion] \label{def:assert}
For every property assertion $\mathtt{P}$, $\neg \mathtt{P}$ is added as
an SMT assertion.
\end{definition}

\subsection{Program Order}
The encoding does not constrain the program order that a point-to-point operation happens before a barrier issued later on an identical process in most cases because a barrier does not enforce the completion of a point-to-point operation. There is only one situation that an operation must be completed before issuing a barrier: a receive must be witnessed by its nearest-enclosing wait before issuing the barrier. As such, such program order that the nearest-enclosing wait happens before the barrier has to be enforced.

\paragraph*{Step 1} For each process, if there are sequential send
operations, say $\mathtt{S}$ and $\mathtt{S^\prime}$, from that task
to a common endpoint, $e_\mathtt{S} = e_\mathtt{S^\prime}$, then those
sends must follow program order: $\mathit{order}_\mathtt{S}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{S^\prime}$.

\paragraph*{Step 2} For each process, if there is a sequential order over the nearest-enclosing wait for a receive operation and a send operation, say $\mathtt{W}$ and $\mathtt{S}$, then they must follow program order: $\mathit{order}_\mathtt{W}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{S}$.

\paragraph*{Step 3} For each process, if there are sequential receive
operations, say $\mathtt{R}$ and $\mathtt{R^\prime}$, in that task
on a common endpoint, $e_\mathtt{R} = e_\mathtt{R^\prime}$, then those
receives must follow program order: $\mathit{order}_\mathtt{R}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{R^\prime}$.

\paragraph*{Step 4} For every receive \texttt{R} and its nearest
enclosing wait \texttt{W}, $\mathit{order}_\mathtt{R}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{W}$.

\paragraph*{Step 5} For any receive \texttt{R} that has a nearest-enclosing barrier \texttt{B} and a nearest-enclosing wait \texttt{W}, they must follow the program order:
$\mathit{order}_\mathtt{W}$ $\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{B}$.

\paragraph*{Step 6} For any barrier $\mathtt{B}$ that has an operation $\mathtt{O}$ issued after it, they must follow the program order: $\mathit{order}_\mathtt{B}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{O}$.

\paragraph*{Step 7} For any pair of sends $\mathtt{S}$ and
$\mathtt{S'}$ on common endpoints, $e_{\mathtt{S}}=e_{\mathtt{S'}}$,
such that
$\mathit{order}_\mathtt{S}\ \mathrm{\prec_\mathtt{HB}}\ \mathit{order}_\mathtt{S'}$,
then those sends must be received in the same order:
$M_{\mathtt{S}}\ \mathrm{\prec_{\mathtt{HB}}}\ M_{\mathtt{S'}}$.



%: DO WE MISS W(R) <_HB S IN A SEQUENTIAL ORDER for infinite buffering?


%: WHAT IF A SEND IS ISSUED BEFORE A RECEIVE or A RECEIVE IS ISSUED BEFORE A SEND IN AN IDENTIAL PROCESS?
%:under zero buffer, each process model should be ordered strictly.



%: show encoding for figure 4

\subsection{Zero-Buffering}
%: show semantics of zero buffer

Under an infinite buffering setting, the MPI semantics allows a message to be buffered in system runtime. Consequently, the message comes later may be received first if the message buffered is not received immediately with respect to the runtime behavior. Our encoding so far focuses on infinite-buffering, therefore, does not have a direct ordering rule for two racing messages to an identical process. 
%:TODO 
. In contrast, the zero buffer setting restricts the buffer size to zero, so each message must be passed substantially once it is sent out. To encode zero-buffering setting, we need to refine our existing ordering rules in such a way that each issued send operation must be received immediately. 

%\begin{definition}[Racing Messages in Zero-Buffering]
%Under zero-buffering, a send \texttt{S} matches a receive \texttt{R} if and only if
%\begin{equation*}
%\forall S'\neq S \cdot( (S' \mathrm{\prec_\mathtt{HB}} S) \wedge (R \mathrm{\prec_\mathtt{HB}} S') 
%                         \vee ((S' \mathrm{\prec_\mathtt{HB}} R) \wedge (S \mathrm{\prec_\mathtt{HB}} S')))
%\end{equation*}
%\end{definition}


%: definition on happens-before* : o_s = o_r -1 
The algorithm defines the \textit{happens-before} relation as a logic expression of two integer variables. It can be extended to a more restricted rule based on the theory of integers, we call \textit{happens-befoore*}, to encode the zero-buffering semantics. 

\begin{definition}[Happens-Before*]
The \emph{Happens-Before*} $(\mathtt{HB*})$ relation over two consecutive event orders $\mathit{order}_\mathtt{e1}$ and $\mathit{order}_\mathtt{e2}$, denoted as
$\mathit{order}_\mathtt{e1} \mathrm{\prec_\mathtt{HB*}} \mathit{order}_\mathtt{e2}$, is a partial order constraint with a logic formula $\mathit{order}_\mathtt{e1} =  \mathit{order}_\mathtt{e2} - 1$.
\label{def:hb*}
\end{definition}

The $(\mathtt{HB*})$ relation over two operations $A$ and $B$ enforces their partial order has to be resolved in a consecutive way. 

%: rules to encode zero buffer
The zero-buffering enforces a message must be received immediately once it is sent out. The encoding constrains this behavior in a new match pair encoding (\defref{def:match*}). 

\begin{definition}[Match Pair *] \label{def:match*}
A match pair, $\langle\mathtt{R}, \mathtt{S}\rangle$, for a receive
$\mathtt{R}$ and a send $\mathtt{S}$ corresponds to the constraints:
\begin{compactenum}
\item $M_{\mathtt{R}} = \mathit{order}_{\mathtt{S}}$
\item $M_{\mathtt{S}} = \mathit{order}_{\mathtt{R}}$
\item $e_{\mathtt{R}} = e_{\mathtt{S}}$
\item $src_\mathtt{R} = src_\mathtt{S}$
\item $\mathit{value}_{\mathtt{R}} = \mathit{value}_{\mathtt{S}}$ and
\item $\mathit{order}_{\mathtt{S}}\ \mathrm{\prec_\mathtt{HB*}}\ \mathit{order}_{\mathtt{R}}$
\end{compactenum}
\end{definition}

In \defref{def:match*}, the order over $\mathtt{S}$ and $\mathtt{R}$ is further constrained such that $\mathtt{S}$ happens before $\mathtt{R}$ in a consecutive order.
%: show encoding for figure 1


To adjust to zero-buffering semantics, the encoding needs to replace \emph{Step 1} with \emph{Step 1*}.

\paragraph*{Step 1*} For each process, if there are a send operation and any operation in a sequential order, say $\mathtt{S}$ and $\mathtt{O}$, then 
they must follow program order: $\mathit{order}_\mathtt{S}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{O}$. 

Further, new rules are added in order to 
%: discuss new rules here

\paragraph*{Step 8} For each process, if there is a sequential order over a send operation and a receive operation, say $\mathtt{S}$ and $\mathtt{R}$, then they must follow program order: 
$\mathit{order}_\mathtt{S}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{R}$.

\paragraph*{Step 9} For each process, if there is a sequential order over a send operation and a barrier operation, say $\mathtt{S}$ and $\mathtt{B}$, then they must follow program order: 
$\mathit{order}_\mathtt{S}$
$\prec_\mathtt{HB}$ $\mathit{order}_\mathtt{B}$.

\subsection{Correctness}

\begin{definition}
For all SMT problems, $s$, let $\mathcal{SOL}(s)$ be in $\sigma +
\mathbb{UNSAT}$, where $\sigma$ is a satisfying assignment of
variables to values.
\end{definition}

\begin{definition}[Semantics]
For all programs, $p$, and traces $t$, $\mathcal{SEM}(p, t)$ is either
$\mathbb{BAD}$ or $\mathbb{OK}$.
\end{definition}

\begin{theorem}[Soundness]
For all programs, $p$, and match pair sets, $m$,
$\mathcal{SOL}(\mathcal{SMT}(p, m)) = t \Rightarrow \mathcal{SEM}(p, t) =
\mathbb{BAD}$.
\end{theorem}


\begin{lemma} \label{lem:bogus}
Any match pair $\langle \mathtt{R}, \mathtt{S}\rangle$ used in a
satisfying assignment of an SMT encoding is a valid match pair and
reflects an actual possible MCAPI program execution.
\end{lemma}

\begin{theorem}[Completeness]
For all programs, $p$, and traces, $t$, $\mathcal{SEM}(p, t) =
\mathbb{BAD} \Rightarrow \exists m . \mathcal{SOL}(\mathcal{SMT}(p,
m)) = t$.
\end{theorem}

\begin{theorem}[Approximation]
Give two match pair sets $m$ and $m'$, $m \subseteq m' \Rightarrow \mathcal{SOL}(\mathcal{SMT}(p, m))
  \sqsubseteq \mathcal{SOL}(\mathcal{SMT}(p, m'))$, where
  $\mathbb{UNSAT} \sqsubseteq \sigma$.
\end{theorem}

\begin{figure}[history]
\begin{center}
\setlength{\tabcolsep}{2pt}
\small \begin{tabular}[t]{l}
h00: P2 $\mathtt{S(to\ P0, "4", \&h5)}$ \\
h01: P2 $\mathtt{S: 4}$\\
h02: P0 $\mathtt{R(from\ P2, A, \&h1)}$ \\
h03: P0 $\mathtt{R: 4}$ \\
h04: P2 $\mathtt{S(to\ P1, "Go", \&h6)}$ \\
h05: P2 $\mathtt{S: Go}$ \\
h06: P1 $\mathtt{R(from\ P2, C, \&h3)}$ \\
h07: P1 $\mathtt{R: Go}$ \\
h08: P1 $\mathtt{S(to\ P0, "1", \&h4)}$ \\
h09: P1 $\mathtt{S: 1}$ \\
h10: P0 $\mathtt{R(from\ P1, B, \&h2)}$ \\
h11: P0 $\mathtt{R: 1}$ \\
\end{tabular}
\end{center}
\caption{The legal history of the MPI program execution in \figref{fig:mpi}}
\label{fig:history}
\end{figure}

So far, we have proved the infinite-buffer encoding is correct. Next, we want to prove the correctness of the zero-buffer encoding. As shown earlier, the encoding extends the rules for match pair and program order under zero buffer semantics. An essential rule is the extended $\mathtt{HB*}$ relation that constrains a send and a matching receive in a consecutive order. Based on this rule, any execution resolved by an SMT solver follows the partial order that a send and a receive are ordered alternately. As such, two consecutive receives or sends are not allowed. This assumption on the zero-buffer encoding is inspired by Concurrent CBMC \cite{}. To be precise, the encoding asks an SMT solver to consider only one form of executions so that the program state space is reduced. One may argue that such a encoding dose not cover all possible message communications without considering arbitrary order over sends and receives. However, we claim that all possible message communications are encoded for zero-buffering semantics. To prove that, we can show these resolved executions actually represent the behavior of all executions that are not covered by the encoding. Before stating the theorem for coverage, we must define a few terms.

An operation can be split into two events: invocation and response, that confirm it is issued and completed, respectively. The following definition of legal history relies on both events.

\begin{definition}[Legal History]
A legal history of an MPI program is a sequence of send and receive operations in form of alternate invocation and response. This sequence obeys the partial order defined in the encoding and can be executed to completion by the operational semantics (defined in the prior work \cite{}). 
\end{definition}

As an example, \figref{fig:history} shows a legal history of the program in \figref{fig:mpi}. In this history, a invocation and a response are alternately ordered for a single send or receive operation. Each invocation, such as line \texttt{h00}, consists of a process rank and a method call. Each response, such as line \texttt{h01}, consists of a process rank and a return value. The return value could be a message sent out for a send operation, or a message received for a receive operation.

We assume that two legal histories of an MPI program are equivalent based on the following definition. To be precise, two histories should receive an identical sequence of messages on each process. In other words, each history should be equal to the other when they are projected to the receive responses on each process. 

\begin{definition}[Equivalency Relation]
Two legal histories, say $H$ and $L$, are equivalent, denoted as $H$ $\sim$ $L$, if and only if their projections to the receive responses on a single process $p$, $H | p$ and $L | p$ respectively, have an identical sequence of return values.
\end{definition}

It is easy to prove that this relation is reflexive, symmetric and transitive. Therefore, it is enough to identify the equivalent classes among all legal histories that can be executed to completion by the operational semantics in \cite{}. The following theorem further states that a representative exists for each equivalent class. 

\begin{theorem}
In zero-buffering semantics, there exists a legal history in each equivalent class that orders each send and its matching receive alternately. 
\end{theorem}

\begin{proof}
prove 
\end{proof}



 


