\section{Example}\label{sec:example}
\subsection{Point-To-Point Communication}
It is worthwhile to take a look at a simple scenario with several MPI calls. Consider \figref{fig:mpi} as an example, it is a trace program including point-to-point operations, i.e., sends and receives, for message communication. Branch and loop structures are not included because the program itself is generated by an execution trace. The notation ``\texttt{---}" represents the operations such as ``\texttt{MPI\_Init}" and ``\texttt{MPI\_Finalize}" that are necessary but not interesting in our discussion. The shorthand notations are used in this program. To be precise, $\mathtt{S}$ represents a non-blocking send; $\mathtt{R}$ represents a non-blocking receive; and $\mathtt{W}$ represents a wait for a send or a receive. For this MPI scenario, line \texttt{00} and \texttt{02} on process $0$ receives two messages in variables $A$ and $B$,respectively; line \texttt{10} on Process $1$ receives a message in variable $C$ from process $2$ and then sends a message ``\texttt{1}" at line \texttt{12} to process $0$; and line \texttt{20} and \texttt{22} on process $2$ sends a message ``\texttt{4}" and ``\texttt{Go}" to process $0$ and $1$, respectively. In addition, process $0$ asserts that variable $A$ is equal to ``\texttt{4}" at line \texttt{04}. A wait operation witnesses the completion of the associated send or receive by allowing the buffer to be free to use. Each receive on process $0$ may receive a message from any source, specified in the parameter ``from", thus is called a wildcard receive. The receive on process $1$ specifies a explicit source thus is called a deterministic receive. \textit{Given this scenario, is variable $A$ assigned a corresponding value in all possible executions so the property on process $0$ does not fail?}

%\subsection{Outcoming Traces}
\begin{figure}[b]
\begin{center}
\setlength{\tabcolsep}{2pt}
\small \begin{tabular}[t]{l}
20 $\mathtt{S(to\ P0, "4", \&h5)}$ \\
21 $\mathtt{W(\&h5)}$\\
\hline
00 $\mathtt{R(from\ P2, A, \&h1)}$ \\
01 $\mathtt{W(\&h1)}$ \\
\hline
22 $\mathtt{S(to\ P1, "Go", \&h6)}$ \\
23 $\mathtt{W(\&h6)}$ \\
\hline
10 $\mathtt{R(from\ P2, C, \&h3)}$ \\
11 $\mathtt{W(\&h3)}$ \\
12 $\mathtt{S(to\ P0, "1", \&h4)}$ \\
13 $\mathtt{W(\&h4)}$ \\
\hline
02 $\mathtt{R(from\ P1, B, \&h2)}$ \\
03 $\mathtt{W(\&h2)}$ \\
04 $\mathtt{assume(B > 0)}$
05 $\mathtt{assert(A == 4)}$ \\
\hline
\end{tabular}
\end{center}
\caption{A feasible execution trace of the MPI program execution in \figref{fig:mpi}}
\label{fig:trace1}
\end{figure}

\figref{fig:trace1} is a feasible execution of the events in \figref{fig:mpi}. The source parameter in each wildcard receive is replaced with a explicit process rank indicating each message is deterministically delivered. In this trace, process $2$ first sends the message ``\texttt{4}" which is immediately received by process $0$ in variable $A$. Process $2$ then sends another message ``\texttt{Go}" which is received by process $1$ in variable $C$. After the receive on process $1$ is completed, the send at line \texttt{12} is able to be issued. Finally, the message ``\texttt{1}" is received by process $0$ in variable $B$. The control flow that ``\texttt{B > 0}" is captured and the assert at line \texttt{05} does not fail. As shown in the trace, each message is immediately received by a matching receive. This communication topology is enforced by zero buffer semantics, but is not required by infinite buffer semantics.

\begin{figure}[t]
\begin{center}
\setlength{\tabcolsep}{2pt}
\small \begin{tabular}[t]{l}
20 $\mathtt{S(to\ P0, "4", \&h5)}$ \\
21 $\mathtt{W(\&h5)}$\\
22 $\mathtt{S(to\ P1, "Go", \&h6)}$ \\
23 $\mathtt{W(\&h6)}$ \\
\hline
10 $\mathtt{R(from\ P2, C, \&h3)}$ \\
11 $\mathtt{W(\&h3)}$ \\
12 $\mathtt{S(to\ P0, "1", \&h4)}$ \\
13 $\mathtt{W(\&h4)}$ \\
\hline
00 $\mathtt{R(from\ P1, A, \&h1)}$ \\
01 $\mathtt{W(\&h1)}$ \\
02 $\mathtt{R(from\ P2, B, \&h2)}$ \\
03 $\mathtt{W(\&h2)}$ \\
04 $\mathtt{assume(B > 0)}$
05 $\mathtt{assert(A == 4)}$ \\
\hline
\end{tabular}
\end{center}
\caption{A second feasible execution trace of the MPI program execution in \figref{fig:mpi}}
\label{fig:trace2}
\end{figure}

It is possible to execute the same program with another feasible trace (\figref{fig:trace2}). This trace assumes an implementation of infinite buffer semantics. Unlike the first trace, the trace in \figref{fig:trace2} buffers the message ``4" instead of transferring it immediately. The message ``\texttt{Go}" is then sent from process $2$ to process $1$ in variable $C$. After that, the message ``1" on process $1$ is sent out. At this point, the arrival of message ``4" and ``1" are racing and the message ``1" arrives first. As a result, variable $A$ is equal to ``\texttt{1}" and the assertion at line \texttt{04} fails.

As shown above, the message non-determinism is significant and difficult to analyze, especially when infinite buffer semantics are enforced which have more possibilities for message communication. Observe that the second trace is not allowed by zero buffer semantics. The message ``\texttt{4}" must be received immediately once it is sent out. As such, there is no way to resolve the match on the receive at line \texttt{00} and the send at line \texttt{12}.  The assertion at line \texttt{04} should not fail under zero buffer semantics. 


\subsection{Collective Communication}
%\begin{figure}[c]
%\begin{center}
%\setlength{\tabcolsep}{2pt}
%\small \begin{tabular}[t]{l}
%20 $\mathtt{S(to\ P0, "4", \&h5)}$ \\
%21 $\mathtt{W(\&h5)}$\\
%22 $\mathtt{\underline{B(comm)}}$\\
%\hline
%10 $\mathtt{\underline{B(comm)}}$\\
%\hline
%00 $\mathtt{R(from\ P2, A, \&h1)}$ \\
%01 $\mathtt{W(\&h1)}$ \\
%02 $\mathtt{\underline{B(comm)}}$\\
%\hline
%23 $\mathtt{S(to\ P1, "Go", \&h6)}$ \\
%24 $\mathtt{W(\&h6)}$ \\
%\hline
%11 $\mathtt{R(from\ P2, C, \&h3)}$ \\
%12 $\mathtt{W(\&h3)}$ \\
%13 $\mathtt{S(to\ P0, "1", \&h4)}$ \\
%14 $\mathtt{W(\&h4)}$ \\
%\hline
%03 $\mathtt{R(from\ P1, B, \&h2)}$ \\
%04 $\mathtt{W(\&h2)}$ \\
%05 $\mathtt{assert(A == 4);}$ \\
%\hline
%\end{tabular}
%\end{center}
%\caption{A feasible execution trace of the MPI program execution in \figref{fig:mpi_barrier}}
%\label{fig:trace3}
%\end{figure}

\examplefigoneB

Consider the collective operations, the analysis of MPI semantics is even harder. There are various categories of collective operations such as barriers and broadcasts, etc., that synchronize an MPI program in a common way. The operations in each category are issued in a group. Each group member blocks the execution of a process unless all members are completed. We take the barrier as an example in the following discussion.
\figref{fig:mpi_barrier} refines the program in \figref{fig:mpi} by inserting a set of barriers. Each barrier, specified by a group identity, i.e., ``\texttt{comm}", sits on a specific location of a process. 
Given such a change on the original MPI scenario, it is interesting to discuss the same question above: is the assertion going to fail?  

Assume infinite buffer semantics are enforced, each process waits on a barrier until all the barriers are matched. As a result, the messages ``\texttt{Go}" and ``1" have no way to be sent out unless the message ``\texttt{4}" is received by process $0$ that ensures the variable $A$ is equal to $0$. Consequently, the assertion on process $0$ does not fail. Given this scenario, It is easy to find out the message communication is affected by barriers. The next section presents an algorithm that encodes the semantics of both point-to-point operations and collective operations into an SMT problem. The encoding also constrains the assertion violation problem and can be solved by an SMT solver such as Yices \cite{dutertre:CAV06} or Z3 \cite{demoura:tacas08}.





