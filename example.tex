\section{Example}\label{sec:example}
\subsection{Point-To-Point Communication}
To understand MPI semantics, it is worthwhile to take a look at a simple scenario with several MPI calls. Consider \figref{fig:mpi} as an example. It is an MPI program including point-to-point operations (i.e., sends and receives) for message communication. Branch and loop structures are not included because the program itself is generated by an execution trace. The notation ``\texttt{---}" represents the operations such as ``\texttt{MPI\_Init}" and ``\texttt{MPI\_Finalize}" that are necessary but not interesting in our discussion. Shorthand notation is used in this program. To be precise, $\mathtt{S}$ represents a non-blocking send; $\mathtt{R}$ represents a non-blocking receive; and $\mathtt{W}$ represents a wait for a send or a receive. For this MPI scenario, lines \texttt{00} and \texttt{02} on process $0$ receive two messages in variables $A$ and $B$ respectively; line \texttt{10} on Process $1$ receives a message in variable $C$ from process $2$ and then sends a message ``\texttt{1}" at line \texttt{12} to process $0$; and lines \texttt{20} and \texttt{22} on process $2$ send messages ``\texttt{4}" and ``\texttt{Go}" to process $0$ and $1$, respectively. In addition, process $0$ asserts that variable $A$ is equal to ``\texttt{4}" at line \texttt{05}. A wait operation witnesses the completion of the associated send or receive and indicates the buffer is free to use. Each receive on process $0$ may receive a message from any source specified in the parameter ``from". In this example, it is a wildcard receive. The receive on process $1$ specifies a explicit source thus is called a deterministic receive. Given that wildcard receive operations may receive messages non-deterministically, a question would be: \textit{Does the assertion on process $0$ fail after the messages are delivered?}

%\subsection{Outcoming Traces}
\begin{figure}[b]
\begin{center}
\setlength{\tabcolsep}{2pt}
\small \begin{tabular}[t]{l}
20 $\mathtt{S(to\ P0, ``4", \&h5)}$ \\
21 $\mathtt{W(\&h5)}$\\
\hline
00 $\mathtt{R(from\ P2, A, \&h1)}$ \\
01 $\mathtt{W(\&h1)}$ \\
\hline
22 $\mathtt{S(to\ P1, ``Go", \&h6)}$ \\
23 $\mathtt{W(\&h6)}$ \\
\hline
10 $\mathtt{R(from\ P2, C, \&h3)}$ \\
11 $\mathtt{W(\&h3)}$ \\
12 $\mathtt{S(to\ P0, ``1", \&h4)}$ \\
13 $\mathtt{W(\&h4)}$ \\
\hline
02 $\mathtt{R(from\ P1, B, \&h2)}$ \\
03 $\mathtt{W(\&h2)}$ \\
04 $\mathtt{assume(B > 0)}$ \\
05 $\mathtt{assert(A == 4)}$ \\
\hline
\end{tabular}
\end{center}
\caption{A feasible execution trace of the MPI program execution in \figref{fig:mpi}}
\label{fig:trace1}
\end{figure}

\figref{fig:trace1} is a feasible execution of the events in \figref{fig:mpi}. The source parameter in each wildcard receive is replaced with a explicit process rank indicating each message is deterministically delivered. In this trace, process $2$ first sends the message ``\texttt{4}" which is immediately received by process $0$ in variable $A$. Process $2$ then sends another message ``\texttt{Go}" which is received by process $1$ in variable $C$. After the receive on process $1$ is completed, the send at line \texttt{12} is able to be issued. Finally, the message ``\texttt{1}" is received by process $0$ in variable $B$. The control flow that ``\texttt{B > 0}" is preserved from the original execution and the assert at line \texttt{05} does not fail. As shown in the trace, each message is immediately received by a matching receive. This communication topology is enforced by zero buffer semantics, but is not required by infinite buffer semantics.

\begin{figure}[t]
\begin{center}
\setlength{\tabcolsep}{2pt}
\small \begin{tabular}[t]{l}
20 $\mathtt{S(to\ P0, ``4", \&h5)}$ \\
21 $\mathtt{W(\&h5)}$\\
22 $\mathtt{S(to\ P1, ``Go", \&h6)}$ \\
23 $\mathtt{W(\&h6)}$ \\
\hline
10 $\mathtt{R(from\ P2, C, \&h3)}$ \\
11 $\mathtt{W(\&h3)}$ \\
12 $\mathtt{S(to\ P0, ``1", \&h4)}$ \\
13 $\mathtt{W(\&h4)}$ \\
\hline
00 $\mathtt{R(from\ P1, A, \&h1)}$ \\
01 $\mathtt{W(\&h1)}$ \\
02 $\mathtt{R(from\ P2, B, \&h2)}$ \\
03 $\mathtt{W(\&h2)}$ \\
04 $\mathtt{assume(B > 0)}$ \\
05 $\mathtt{assert(A == 4)}$ \\
\hline
\end{tabular}
\end{center}
\caption{A second feasible execution trace of the MPI program execution in \figref{fig:mpi}}
\label{fig:trace2}
\end{figure}

It is possible to execute the same program with another feasible trace (\figref{fig:trace2}). This trace assumes an implementation of infinite buffer semantics. Unlike the first trace, the trace in \figref{fig:trace2} buffers the message ``4" instead of transferring it immediately. The message ``\texttt{Go}" is then sent from process $2$ to process $1$ in variable $C$. After that, the message ``1" on process $1$ is sent out. At this point, the arrival of message ``4" and ``1" are racing and the message ``1" arrives first. As a result, the assume at line \texttt{04} is satisfied but the assertion at line \texttt{05} fails.

Message non-determinism is significant and difficult to analyze, especially when infinite buffer semantics are enforced which have more possibilities for message communication. Observe that the second trace is only allowed by zero buffer semantics. The message ``\texttt{4}" must be received immediately once it is sent out. As such, there is no way to match the receive at line \texttt{00} and the send at line \texttt{12}.  The assertion at line \texttt{05} does not fail under zero buffer semantics. 


\subsection{Collective Communication}
%\begin{figure}[c]
%\begin{center}
%\setlength{\tabcolsep}{2pt}
%\small \begin{tabular}[t]{l}
%20 $\mathtt{S(to\ P0, "4", \&h5)}$ \\
%21 $\mathtt{W(\&h5)}$\\
%22 $\mathtt{\underline{B(comm)}}$\\
%\hline
%10 $\mathtt{\underline{B(comm)}}$\\
%\hline
%00 $\mathtt{R(from\ P2, A, \&h1)}$ \\
%01 $\mathtt{W(\&h1)}$ \\
%02 $\mathtt{\underline{B(comm)}}$\\
%\hline
%23 $\mathtt{S(to\ P1, "Go", \&h6)}$ \\
%24 $\mathtt{W(\&h6)}$ \\
%\hline
%11 $\mathtt{R(from\ P2, C, \&h3)}$ \\
%12 $\mathtt{W(\&h3)}$ \\
%13 $\mathtt{S(to\ P0, "1", \&h4)}$ \\
%14 $\mathtt{W(\&h4)}$ \\
%\hline
%03 $\mathtt{R(from\ P1, B, \&h2)}$ \\
%04 $\mathtt{W(\&h2)}$ \\
%05 $\mathtt{assert(A == 4);}$ \\
%\hline
%\end{tabular}
%\end{center}
%\caption{A feasible execution trace of the MPI program execution in \figref{fig:mpi_barrier}}
%\label{fig:trace3}
%\end{figure}

\examplefigoneB

The analysis of MPI semantics is even harder with collective operations because message communication can be affected. There are various types of collective operations such as barriers and broadcasts, etc., that synchronize an MPI program in a common way. Each type of operation is used as a group. Each operation in the group blocks the execution of a process until all the operations are completed. We take barrier as an example in the following discussion.
\figref{fig:mpi_barrier} refines the program in \figref{fig:mpi} by inserting a set of barriers. Each barrier, specified by a group identity (i.e., ``\texttt{comm}") sits on a specific location of a process. 
Given such a change on the original MPI scenario, it is interesting to discuss the same question: is the assertion going to fail?  

Assuming infinite buffer semantics are enforced, each process waits on a barrier until all the barriers are matched. As a result, the messages ``\texttt{Go}" and ``1" have no way to be sent out unless the message ``\texttt{4}" is received by process $0$ that ensures the variable $A$ is equal to $4$. Consequently, the assertion on process $0$ does not fail. As such, the message communication in this example is limited by the barriers. The next section presents an algorithm that encodes the semantics of both point-to-point communication and collective communication into an SMT problem. The encoding can be solved by an SMT solver such as Yices \cite{dutertre:CAV06}, Z3 \cite{demoura:tacas08}, etc.





