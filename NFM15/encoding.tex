\section{SMT Encoding}
The SMT encoding is generated from 1) an execution trace of an MPI program that includes a sequence of events (i.e., point-to-point operations, collective operations, control flow assumptions and property assertions); and 2) a set of possible match pairs for message communication. Intuitively, a match pair is a coupling of a send and a receive. Taking \figref{fig:mpi} as an example, each receive on process \texttt{P0} may be matched with the send at line $l_6$ or the send at line $l_{11}$. The direct use of match pairs simplifies reasoning about message communication. 

\subsection{Point-To-Point Communication}

The prior work has already defined a few terms that are still helpful to encode the point-to-point communication in this paper. In general, the encoding contains a timestamp $\mathit{time}_\mathtt{e}$ for every event $\mathtt{e}$ in an MPI program. Intuitively, the timestamp is an integer that is able to be ordered.  The order is enforced by \emph{Happens-Before} $(\mathtt{HB})$ operator, denoted as
$\mathrm{\prec_\mathtt{HB}}$, over two events, $\mathtt{e1}$ and $\mathtt{e2}$ respectively, such that $\mathit{time}_\mathtt{e1} <  \mathit{time}_\mathtt{e2}$. The send and receive operations are encoded as tuples. In particular, a send operation $\mathtt{S}$ $=$ $(M_\mathtt{S},$ $\mathit{time}_\mathtt{S},$ $e_\mathtt{S},$ $\mathit{value}_\mathtt{S})$, is a four-tuple of variables. $M_\mathtt{S}$ is the timestamp of the matching receive; $\mathit{time}_\mathtt{S}$ is the timestamp of $\mathtt{S}$; $e_\mathtt{S}$ is the destination endpoint; and $\mathit{value}_\mathtt{S}$ is the transmitted value. Similarly, a receive operation $\mathtt{R}$ $=$ $(M_\mathtt{R},$ $\mathit{time}_\mathtt{R},$ $e_\mathtt{R},$ $\mathit{value}_\mathtt{R},$ $\mathit{time}_{\mathtt{W}_\mathtt{R}})$, is modeled by a five-tuple of variables. $M_\mathtt{R}$ is the timestamp of the matching send; $\mathit{time}_\mathtt{R}$ is the timestamp of $\mathtt{R}$; $e_\mathtt{R}$ is the destination endpoint; $\mathit{value}_\mathtt{R}$ is the received value; and $\mathit{time}_{\mathit{w}_\mathtt{R}}$ is the timestamp of the nearest-enclosing wait. A nearest-enclosing wait is a wait that witnesses the completion of a receive by indicating that the message is delivered and that all the previous receives on the same process issued earlier are completed as well. The message communication topology is encoded as a set of match pairs.

\begin{definition}[Match Pair] \label{def:match}
A match pair, $\langle\mathtt{R}, \mathtt{S}\rangle$, for a receive 
$\mathtt{R}$ $=$ $(M_\mathtt{R},$ $\mathit{time}_\mathtt{R},$ $e_\mathtt{R},$ $\mathit{value}_\mathtt{R},$ $\mathit{time}_{\mathtt{W}_\mathtt{R}})$ and a send $\mathtt{S}$ $=$ $(M_\mathtt{S},$ $\mathit{time}_\mathtt{S},$ $e_\mathtt{S},$ $\mathit{value}_\mathtt{S})$, corresponds to the following constraints:
\begin{compactenum}
\item $M_{\mathtt{R}} = \mathit{time}_{\mathtt{S}}$
\item $M_{\mathtt{S}} = \mathit{time}_{\mathtt{R}}$
\item $e_{\mathtt{R}} = e_{\mathtt{S}}$
\item $\mathit{value}_{\mathtt{R}} = \mathit{value}_{\mathtt{S}}$ 
\item $\mathit{time}_{\mathtt{S}}\ \mathrm{\prec_\mathtt{HB}}\ \mathit{time}_{\mathtt{W}_\mathtt{R}}$
\end{compactenum}
\end{definition}

We then define the potential sends for a receive $\mathtt{R}$, denoted as $\mathrm{Match}(\mathtt{R})$, is a set of all the sends that $\mathtt{R}$ may potentially match. As such, the encoding of point-to-point communication(also defined in the prior work \cite{DBLP:conf/kbse/HuangMM13}) is defined in \figref{fig:ptp}. In this figure, rules $(1)$ -- $(5)$ encodes the program order. Rule $(6)$ encodes the match pairs. Rules $(7)$ and $(8)$ encodes the assumption on control flow and the negation of property assertion respectively. Assume the size of all operations in an MPI program is $\mathcal{N}$, the generated SMT encoding contains $\mathcal{O}(\mathcal{N}^2)$ formulas.

\encodingptp

\subsection{Deterministic Communication}
As the prior work \cite{DBLP:conf/kbse/HuangMM13} does not encode deterministic point-to-point communication that is allowed in MPI semantics, we need to support it in the new encoding. To be precise, the encoding needs to add variables  $src_\mathtt{S}$ and $src_\mathtt{R}$ to the send operation and the receive operation respectively. Intuitively, variable $src_\mathtt{S}$($src_\mathtt{R}$) is the source process endpoint of a message. As such, a send operation $\mathtt{S}$ $=$ $(M_\mathtt{S},$ $\mathit{time}_\mathtt{S},$ $e_\mathtt{S},$ $src_\mathtt{S},$ $\mathit{value}_\mathtt{S})$,  is now a five-tuple of variables. A receive operation $\mathtt{R}$ $=$ $(M_\mathtt{R},$ $\mathit{time}_\mathtt{R},$ $e_\mathtt{R},$ $src_\mathtt{R},$ $\mathit{value}_\mathtt{R},$ $\mathit{time}_{\mathtt{W}_\mathtt{R}})$, is now a six-tuple of variables. In particular, we enforce variable $src_\mathtt{R} = -1$ for a wildcard receive $\mathtt{R}$. In addition, the match pair defined in \defref{def:match} should add a new constraint: 
\[
src_\mathtt{R} = -1 \vee src_\mathtt{S} = src_\mathtt{R}, 
\]
indicating that either $\mathtt{R}$ is a wildcard receive or the source endpoints are matched for $\mathtt{S}$ and $\mathtt{R}$.

\subsection{Collective Communication}
As discussed earlier, the use of any type of collective operation is able to synchronize the program. In addition, some type of collective operation such as broadcast is able to send internal messages among its group, and/or to execute global operations. MPI semantics guarantee that messages generated on behalf of collective operations are not confused with messages generated by point-to-point operations. Therefore, the encoding in this paper puts emphasis on how to reason about the synchronization of collective operations as it may affect point-to-point communication. The internal message passing and the execution of global operations by collective communication can be simply added as SMT constraints to the encoding. In the following discussion, we take barrier as an example to encode the synchronization behavior. The barrier is defined as a group in \defref{def:barrier}. 

\begin{definition}[Barrier]\label{def:barrier}
The occurrence of a group of barriers, $B$ = $\{B_0, B_1, ..., B_n\}$, is captured by a
single timestamp, $\mathit{time}_\mathtt{B}$, that marks when all the members in the group are matched.  
\end{definition}

\begin{figure}[h]
\[
\begin{array}{l|l}
\;\;\;\;\;\;\;\;\mathtt{Process\ 0}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{Process\ 1}\;\;\;\;\;\;\;\; \\
\hline
\;\;\;\;\;\;\;\;\mathtt{\underline{B(comm)}}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{R(from\ P0,A,\&h2)}\;\;\;\;\;\;\;\; \\
\;\;\;\;\;\;\;\;\mathtt{S(to\ P1,``1",\&h1)}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{\underline{B(comm)}}\;\;\;\;\;\;\;\; \\
\;\;\;\;\;\;\;\;\mathtt{W(\&h1)}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{W(\&h2)}\;\;\;\;\;\;\;\; \\
\end{array}
\]
\caption{Message Communication with Barriers} \label{fig:mc_barrier1}
\end{figure}

Even though barriers affect the issuing order of two events, it is hard to determine whether they prevent a send from matching a receive. As an example, the message ``$1$" in \figref{fig:mc_barrier1} may flow into $\mathtt{R}$ even though $\mathtt{R}$ is ordered before the barrier and $\mathtt{S}$ is ordered after the barrier. The wait $\mathtt{W(\&h2)}$ determines the behavior. If the program had issued $\mathtt{W(\&h2)}$ before the barrier, $\mathtt{R}$ would have to be completed before the barrier, meaning the message is delivered. The encoding further defines the nearest-enclosing barrier (\defref{def:nb}) for this type of interaction.

\begin{definition}[Nearest-Enclosing Barrier]\label{def:nb}
For any process $i$, a receive $\mathtt{R}$ has a nearest-enclosing barrier $\mathtt{B}$ if and only if
\begin{compactenum}
\item the nearest-enclosing wait, $\mathtt{W}$, of $\mathtt{R}$ is ordered before $B_i\in B$, and
\item there does not exist any receive $\mathtt{R^\prime}$ on process $i$, with a nearest-enclosing wait, $\mathtt{W^\prime}$, such that 1) $\mathtt{W^\prime}$ is ordered after $\mathtt{W}$; and 2) $\mathtt{W^\prime}$ is ordered before $B_i$.
\end{compactenum}
\end{definition}

Based on the definitions above, The encoding defines two rules for program order in \figref{fig:cc}. Rule $(9)$ only constrains the program order over the nearest-enclosing wait and the nearest-enclosing barrier for a receive. The order over this receive and the nearest-enclosing barrier is not constrained. For rule $(10)$, a barrier has to happen before any operation ordered after it. 

Given the definitions above, we are able encode the sample program in \figref{fig:mpi} with infinite buffer configuration. \figref{fig:mpiencoding}(a) is the encoding containing a set of formulas. Resolving these formulas by a common SMT solver(e.g., Z3) produces a violating trace(e.g., trace $\pi_1$ mentioned earlier). 

%Intuitively, the program order, match pairs and program properties are encoded with respect to infinite buffer semantics. The encoding does not add too many constraints for program order because infinite buffer semantics give more possibilities for message communication. The much pair encoding further gives all possible choices for message communication. The program properties are also encoded so the solver is capable of finding assertion violation for the program.

\encodingcc

\exampleencoding

\subsection{Zero Buffer Encoding}
The zero buffer semantics enforce messages to be delivered once they are sent. The prior work \cite{DBLP:conf/kbse/HuangMM13} provides a complicated method that is hard to encode. This paper presents a new zero buffer encoding technique that is easy to reason about and encode. To be precise, we need to refine the existing rules for the infinite buffer encoding. The significant change is to order a send immediately preceding the matching receive in a match pair. Therefore, the match pair is defined in \defref{def:match*}. 

\begin{definition}[Match Pair *] \label{def:match*}
A match pair, $\langle\mathtt{R}, \mathtt{S}\rangle^*$, for a receive
$\mathtt{R}$ $\mathtt{R}$ $=$ $(M_\mathtt{R},$ $\mathit{time}_\mathtt{R},$ $e_\mathtt{R},$ $\mathit{value}_\mathtt{R},$ $\mathit{time}_{\mathtt{W}_\mathtt{R}})$ and a send $\mathtt{S}$ $=$ $(M_\mathtt{S},$ $\mathit{time}_\mathtt{S},$ $e_\mathtt{S},$ $\mathit{value}_\mathtt{S})$, corresponds to the constraints:
\begin{compactenum}
\item $M_{\mathtt{R}} = \mathit{time}_{\mathtt{S}}$
\item $M_{\mathtt{S}} = \mathit{time}_{\mathtt{R}}$
\item $e_{\mathtt{R}} = e_{\mathtt{S}}$
\item $src_\mathtt{R} = -1 \vee src_\mathtt{S} = src_\mathtt{R}$
\item $\mathit{value}_{\mathtt{R}} = \mathit{value}_{\mathtt{S}}$ 
\item $\mathit{\textbf{time}}_{\mathtt{\textbf{S}}}\ = \ \mathit{\textbf{time}}_{\mathtt{\textbf{R}}} - \mathit{\textbf{1}}$
\end{compactenum}
\end{definition}

Intuitively, the consecutive order over a send and the matching receive is defined in the bold rule $6$ of \defref{def:match*}. Any resolved execution strictly alternates sends and receives as defined in \defref{def:alternate}. 

\begin{definition}[Strict Alternating]\label{def:alternate}
A set of sends, $S$, and a set of receives, $R$, are strictly alternated if and only if each send in $S$ immediately precedes the matching receive in $R$ and each receive in $R$ immediately follows the matching send in $S$.
%:
\end{definition}
To further constrain the program order for zero buffer semantics, new rules are added: on each process a send happens before a receive that is issued later (rule $(11)$); and similarly, on each process a send happens before a barrier that is issued later (rule $(12)$). In addition, Rule $(1^*)$ replaces rule $(1)$ as zero buffer semantics do not allow a new send to be issued before the pending send is completed on a common process. Rule $(6^*)$ replaces rule $(6)$ to enforces strict alternating for every send and its matching receive.

\encodingzb

\figref{fig:mpiencoding}(b) illustrates the zero buffer encoding for the sample program in \figref{fig:mpi}. It is very similar to the infinite buffer encoding in \figref{fig:mpiencoding}(a), only the program order on each process is highly constrained, and sends and receives are strictly alternated for match pairs. This encoding is able to detect assertion violations for zero buffer semantics. Further, the encoding strategy can be extended to detect zero buffer incomparability. 

\subsection{Zero Buffer Compatibility}
A zero buffer incompatible program must deadlock. Notice that the input trace of our encoding is generated from infinite buffer semantics, the match pairs may not be resolved under zero buffer semantics. Taking the program in \figref{fig:mpi} as an example, there is no way to resolve the strict alternating relation for the first send of each process and its matching receive. As such, the zero buffer encoding for these match pairs is unsatisfiable. Therefore, the zero buffer compatibility is checked by merely encoding and resolving the partial order constraints that are allowed by zero buffer semantics. The constraints for user-provided assertions defined in rule $(8)$ are simply removed because they are irrelevant for this purpose. If the encoding is satisfiable, the compatibility of this scenario is proved; otherwise, it can not be executed because the execution would deadlock. The correctness discussed later further demonstrates this solution is capable of precisely checking zero buffer compatibility of an MPI program.

Notice that a deadlock program may not be zero buffer incompatible. Intuitively, deadlock is caused by various reasons: orphan send or receive that is never matched, dependency circuit existing in message communication topology, and improper implementation of collective operations, etc. Given that the MPI semantics are complicated, it is hard to precisely and efficiently check deadlock for an MPI program \cite{}. The approach in this paper, however, focuses on zero buffer compatibility and is able to check it with an SMT encoding in quadric size.  


%: add discussion on zero buffer encoding of sample program here
%: add discussion on zero buffer compatibility here


%: Correctness
\subsection{Correctness}
The prior work \cite{DBLP:conf/kbse/HuangMM13} proves the encoding technique is correct for wildcard receives. To be precise, the encoding is sound, meaning any satisfying schedule resolved by the encoding reflects an actual violating execution. It is also complete, meaning any actual violation can be resolved in a satisfying schedule by the encoding. 

To prove the new technique for MPI semantics is also correct, we rely on the existing soundness and completeness proofs. The soundness proof is consistent with the prior work for two aspects: 1) the program order is precisely constrained in the encoding; 2) any match pair used in a resolved satisfying schedule is valid. The deterministic receive operations and the collective operations for the new encoding do not violate both aspects. To be precise, the first aspect still exists because the program order for deterministic receive operations and collective operations are precisely defined in the new encoding. The second aspect is also true because the set of match pairs is given as input, which is not affected by deterministic receive operations and collective operations. 

The completeness proof of the new technique is similar to the prior work that uses the operational semantics to simulate the encoding during its operation to ensure that the two make identical conclusions. To prove the new encoding is complete, the operational semantics are extended to support deterministic receive operations and collective operations. A simulation of the extended operational semantics is then able to prove that any behavior of MPI semantics is encoded by the new technique in this paper. Because the soundness and the completeness for the new encoding are both proved, we conclude that the encoding is correct for MPI semantics. 

In addition to the soundness and completeness, the improved zero buffer encoding needs to prove the coverage of message communication. As shown earlier, the zero buffer encoding extends the rules for match pair and program order. The rules constrain the order over a send and the matching receive in a match pair to be consecutive. Based on this rule, the encoding assumes that sends and receives strictly alternate in any resolved execution without message non-determinism as in \defref{def:alternate}. This assumption is inspired by the Threaded-C Bounded Model Checking (TCBMC) that assumes each lock operation and its paired unlock operation is ordered alternatingly in any execution \cite{DBLP:conf/cav/RabinovitzG05}. Similarly, the encoding in this paper detects assertion violations by only considering schedules that strictly alternate sends and receives in an execution. 

This strict alternating assumption leads to an important proof obligation: how do we know this encoding covers all possible ways of message communication in zero buffer semantics? We prove a theorem later that asserts that strict alternating sequences of sends and receives cover the message communication that may occur in any execution under zero buffer semantics. Before that, we need to define a few terms.

\begin{definition}[Method Invocation]\label{def:invocation}
A invocation of a method, $M$, with a list of specific values of arguments, $(args\ \cdots)$, on process $P$, denoted as $P:M_\mathit{In}(args\ \cdots)$, is a event that occurs when $M$ is invoked. 
\end{definition}

\begin{definition}[Method Response]\label{def:response}
A response of a method, $M$, with a specific return value, $resp$, on process $P$, denoted as $P:M_\mathit{Re}(resp)$, is a event that occurs when $M$ returns. 
\end{definition}

Based on \defref{def:invocation} and \defref{def:response}, an operation is split into two events: invocation and response. The invocation asserts the issuing of an operation with concrete arguments. The response asserts the completion of an operation with a concrete return value. The next definition relies on both method invocation and method response. 

\begin{definition}[History]\label{def:history}
For an MPI program, let $\mathcal{H}$ be a history with a total order over method invocations and method responses for a set of send operations, $\mathbb{S}$, a set of receive operations, $\mathbb{R}$, and a set of barriers, $\mathbb{B}$.
\end{definition}

Based on \defref{def:history}, we further define the legal history as follows.

\begin{definition}[Legal History]\label{def:legal}
A history, $\mathcal{H}$, for an MPI program is legal if the partial order over any pair of events in $\mathcal{H}$ is allowed by MPI semantics.
\end{definition}

A legal history defined in \defref{def:legal} represents a total order over events for an MPI program. A legal history only takes care of sends, receives and barriers because only they matter to message communication. In other words, the events in a legal history can be used to evaluate how messages may flow in a runtime system. Since the arguments are concrete in any method invocation and the return value is also concrete in any method response, a legal history corresponds to a precise resolution of message communication. To find a feasible message communication for a receive $R$, one only needs to search through the preceding events and find a send $S$ that matches the endpoints of $R$ and obeys the non-overtaking order for all the sends to the common destination endpoint. The legal history asserts that the partial order over any pair of events is allowed by MPI semantics. In particular, if a legal history is allowed by zero buffer semantics, we call it zero-buffer legal history.

To prove the theorem later, we need to compare two legal histories for equivalence. The equivalence relation relies on the following definitions for projections. 

\begin{definition}[Projection To Process]\label{def:projection_process}
A projection of a legal history, $\mathcal{H}$, to a process, $P$, denoted as $\mathcal{H} | _P$, is a sequence of all the events on process $P$ in $\mathcal{H}$, such that the order over any pair of events in $\mathcal{H} | _P$ is identical as in $\mathcal{H}$.
\end{definition}

\begin{definition}[Projection To Receive Response]\label{def:projection_receive}
A projection of a legal history, $\mathcal{H}$, to the receive responses, $\mathbb{R}_r$, denoted as $\mathcal{H} | _{\mathbb{R}_r}$, is a sequence of receive responses in $\mathcal{H}$ such that the order over any pair of receive responses in $\mathcal{H} | _{\mathbb{R}_r}$ is identical as in $\mathcal{H}$.
\end{definition}

Based on \defref{def:projection_process} and \defref{def:projection_receive}, a legal history can be further projected to the receive responses $\mathbb{R}_r$ on process $P$ that produces a sequences of receive responses on process $P$. We use $\mathcal{H} | _{\mathbb{R}_r,P}$ to represent this projection. The equivalence relation relies on $\mathcal{H} | _{\mathbb{R}_r,P}$.

\begin{definition}[Equivalence Relation]\label{def:er}
Two legal histories for an MPI program, say $\mathcal{H}$ and $\mathcal{L}$ respectively, are equivalent, denoted as $\mathcal{H}$ $\sim$ $\mathcal{L}$, if and only if their projections to the receive responses $\mathbb{R}_r$ and each single process $P$, $\mathcal{H} | _{\mathbb{R}_r,P}$ and $\mathcal{L} | _{\mathbb{R}_r,P}$ respectively, agree on the return values of $\mathbb{R}_r$.
\end{definition}

%We further use the notation $\not\sim$ for two legal histories $\mathcal{H}$ and $\mathcal{L}$ when they are not equivalent. 
The following lemma is essential to proving that \defref{def:er} is a valid equivalence relation. \defref{def:reachable} defines the reachable set of legal histories for an MPI program. 

\begin{definition}[Reachable Legal Histories]\label{def:reachable}
For an MPI program, $\mathit{p}$, let $\mathcal{RS}(\mathit{p})$ be a set of all the legal histories that is reachable from $\mathit{p}$.
\end{definition}

\begin{lemma}[validity of equivalence relation]\label{lemma:rst}
The $\sim$-operator is an equivalence relation over the set of all legal histories.
\end{lemma}

\begin{proof}
Consider three legal histories, $\mathcal{H}, \mathcal{L}, \mathcal{T}$ $\in$ $\mathcal{RS}(\mathit{p})$, for an MPI program, $\mathit{p}$, the equivalence relation in \defref{def:er} is reflexive, symmetric and transitive. 
\begin{compactenum}
\item $\mathcal{H}$ $\sim$ $\mathcal{H}$. The reflexivity is true because the projection $\mathcal{H} | _{\mathbb{R}_r,P}$ to the receive responses $\mathbb{R}_r$ on any process $P$, is identical to the projection of itself.
\item $\mathcal{H}$ $\sim$ $\mathcal{L}$ then $\mathcal{L}$ $\sim$ $\mathcal{H}$. The symmetry is also true because $\mathcal{H}$ $\sim$ $\mathcal{L}$ and $\mathcal{L}$ $\sim$ $\mathcal{H}$ both indicate that the projections $\mathcal{H} | _{\mathbb{R}_r,P}$ and $\mathcal{L} | _{\mathbb{R}_r,P}$ to the receive responses $\mathbb{R}_r$ on any process $P$ agree on the return values of $\mathbb{R}_r$. 
\item $\mathcal{H}$ $\sim$ $\mathcal{L}$ and $\mathcal{L}$ $\sim$ $\mathcal{T}$ then $\mathcal{H}$ $\sim$ $\mathcal{T}$. As for the transitivity, for all the receive responses $\mathbb{R}_r$ on any process $P$ in the MPI program, $\mathcal{H}$ $\sim$ $\mathcal{L}$ indicates that the projections $\mathcal{H} | _{\mathbb{R}_r,P}$ and $\mathcal{L} | _{\mathbb{R}_r,P}$ to the receive responses $\mathbb{R}_r$ on any process $P$ agree on the return values of $\mathbb{R}_r$. Further, $\mathcal{L}$ $\sim$ $\mathcal{T}$ indicates that $\mathcal{L} | _{\mathbb{R}_r,P}$ is also identical to $\mathcal{T} | _{\mathbb{R}_r,P}$. Therefore, $\mathcal{H} | _{\mathbb{R}_r,P}$ is identical to $\mathcal{T} | _{\mathbb{R}_r,P}$. Since $P$ is an arbitrary process in the MPI program and $\mathbb{R}_r$ do not change on $P$, thus $\mathcal{H}$ $\sim$ $\mathcal{T}$ is implied.
\end{compactenum}
Based on the reflexivity, symmetry and transitivity, this equivalence relation is able to partition the reachable set of legal histories, $\mathcal{RS}(\mathit{p})$, therefore identifies the equivalent classes among $\mathcal{RS}(\mathit{p})$.
\end{proof}

Based on \lemmaref{lemma:rst}, we further use $\mathcal{E}$$(\mathcal{RS}(\mathit{p}))$ to represent the equivalent classes for all the legal histories in $\mathcal{RS}(\mathit{p})$. If a equivalent class includes zero-buffer legal histories, we call it zero-buffer available equivalent class. The following theorem states that a representative exists for each zero-buffer available equivalent class. 

%\begin{definition}[Equivalent Class]\label{def:equal_class}
%A set of legal histories, $\mathit{set_H}$, is a equivalent class, denoted as $\mathcal{E}$$(\mathit{set_H})$, if and only if,
%\begin{compactenum}
%\item for any two legal history, $L,T$ $\in$ $\mathit{set_H}$, $\mathcal{L}$ $\sim$ $\mathcal{T}$;
%\item for any legal history, $L$ $\notin$ $\mathit{set_H}$, and any legal history, $T$ $\in$ $\mathit{set_H}$, $\mathcal{L}$ $\not\sim$ $\mathcal{T}$.
%\end{compactenum}
%\end{definition}

\begin{theorem}
For any MPI program, $\mathit{p}$, each zero-buffer available equivalent class, $\mathit{E}$ $\in$ $\mathcal{E}(\mathcal{RS}(\mathit{p}))$, has a representative zero-buffer legal history, $T$, that strictly alternates the sends, $\mathbb{S}$, and the receives, $\mathbb{R}$.
\end{theorem}

\begin{proof}
First, assume there is a legal history $\mathcal{L}$ $\in$ $\mathit{E}$. Second, assume $\mathbb{R}_r$ is a set of all the receive responses, and $\mathbb{S}_r$ is a set of all the send responses. The projection $\mathcal{L} | _{\mathbb{R}_r}$ is a sequence of receive responses that reflects how messages are received in $\mathcal{L}$ for all the processes. Since the message communication is precisely resolved in $\mathcal{L}$, each receive in $\mathbb{R}$ is matched with a send in $\mathbb{S}$. Based on $\mathcal{L}$ and $\mathcal{L} | _{\mathbb{R}_r}$, a new sequence, $\mathcal{T}$, can be produced by two steps: 1) inserting the corresponding receive invocation immediately preceding each receive response; and 2) inserting the invocation and the response of the matching send immediately preceding each receive invocation. Based on those steps, $\mathcal{T}$ strictly alternates $\mathbb{S}$ and $\mathbb{R}$. Further, it obeys the condition defined in \defref{def:legal} for two reasons. First, the consecutive order over a send and the matching receive in $\mathcal{L}$ still exists in $\mathcal{T}$; second, if the matching receive is issued earlier on process $P$, there is no way to execute any operation after the receive on process $P$ until it is matched with a send, therefore, postponing issuing the receive after the matching send does not violate the MPI semantics. Under zero buffer semantics, it is not possible to order a send and the matching receive other than these two situations above. Notice that $\mathcal{T}$ is equivalent to $\mathcal{L}$ as they receive a common sequence of messages on each process. Therefore, for any existing zero-buffer available equivalent class, the procedure above is able to find a representative zero-buffer legal history that strictly alternates sends and receives. 
\end{proof}
