\section{Example}\label{sec:example}
%\subsection{Point-To-Point Communication}
To understand MPI semantics, it is worthwhile to take a look at a simple scenario with several MPI calls. Consider \figref{fig:mpi} as an example. It is a single-path MPI program including point-to-point operations (i.e., sends and receives) for message communication. The notation ``\texttt{---}" represents the operations such as ``\texttt{MPI\_Init}" and ``\texttt{MPI\_Finalize}" that are necessary but not interesting in our discussion. Shorthand notation is used in this program. To be precise, $\mathtt{S}$ represents a non-blocking send; $\mathtt{R}$ represents a non-blocking receive; and $\mathtt{W}$ represents a wait for a send or a receive. The subscript $\mathtt{i,j}$ for $\mathtt{S}$, $\mathtt{R}$ or $\mathtt{W}$ represents the process rank and the event rank within this process respectively.
In general, this program includes three processes running in parallel. Each process communicates only with its immediate neighbors. Wildcard receive is used in the message communication. This wildcard receive is able to receive a message from any source specified in the parameter ``from". The assume operation on process \texttt{P0} ensures this program follows the control flow that ``\texttt{B > 0}". To check if the computation correct, process \texttt{P0} further asserts that variable $A$ is equal to ``\texttt{2}".  

Assume infinite buffer semantics, because of message non-determinism, there would be various execution traces resolved at runtime. We use a sequence of line numbers in \figref{fig:mpi} to represent a feasible program execution. The line number of each wait operation is omitted because we assume each wait is issued immediately after the return of a send or a receive. A feasible execution trace of the sample program in \figref{fig:mpi} is $\pi_1 = l_0l_1l_6l_7l_{10}l_{11}l_2(l_6)l_3(l_{11})l_8(l_1)l_9(l_{10})l_{12}(l_0)l_{13}(l_7)l_4l_5$. The line number sitting in parenthesis represents a issued send that matches a receive that is represented as the line number immediately preceding this parenthesis. For example, $l_2(l_6)$ means the send at line $l_6$ is issued and finally matches the receive at line $l_2$ at runtime. This trace resolves a possible way to match all sends and receives. The trace gives the message ``\texttt{1}" as being received by process \texttt{P0} in variable $A$. The assume operation is preserved from the original program and the assert at line $l_5$ does not fail. The assert could be also violated if messages are communicated in another way. An example trace is $\pi_2 = l_0l_1l_{10}l_{11}l_6l_7l_2(l_{11})l_3(l_6)l_8(l_1)l_9(l_{10})l_{12}(l_0)l_{13}(l_7)l_4l_5$. In this trace, variable $A$ is assigned ``\texttt{2}" instead of ``\texttt{1}". Therefore, the assert at line $l_5$ fails indicating that the computation in this execution is incorrect. 

To check if the computation is correct in all possible executions derived from the original program, message non-determinism is significant and difficult to analyze, especially when infinite buffer semantics are enforced which have more possibilities for message communication. Zero buffer semantics, on the other hand, only allow a message to be received immediately. Further, if a send is not able to be received in any execution, the program is zero buffer incompatible. The sample program in \figref{fig:mpi} is a zero buffer incompatible program because there is no way to match the first send on any process once this send is issued. Therefore, executing this program under a zero buffer configuration deadlocks. 

%For this MPI scenario, lines \texttt{00} and \texttt{02} on process $0$ receive two messages in variables $A$ and $B$ respectively; line \texttt{10} on Process $1$ receives a message in variable $C$ from process $2$ and then sends a message ``\texttt{1}" at line \texttt{12} to process $0$; and lines \texttt{20} and \texttt{22} on process $2$ send messages ``\texttt{4}" and ``\texttt{Go}" to process $0$ and $1$, respectively. In addition, process $0$ asserts that variable $A$ is equal to ``\texttt{4}" at line \texttt{05}. A wait operation witnesses the completion of the associated send or receive and indicates the buffer is free to use. Each receive on process $0$ may receive a message from any source specified in the parameter ``from". In this example, it is a wildcard receive. The receive on process $1$ specifies a explicit source thus is called a deterministic receive. Given that wildcard receive operations may receive messages non-deterministically, a question would be: \textit{Does the assertion on process $0$ fail after the messages are delivered?}

%\figref{fig:trace1} is a feasible execution of the events in \figref{fig:mpi}. The source parameter in each wildcard receive is replaced with a explicit process rank indicating each message is deterministically delivered. In this trace, process $2$ first sends the message ``\texttt{4}" which is immediately received by process $0$ in variable $A$. Process $2$ then sends another message ``\texttt{Go}" which is received by process $1$ in variable $C$. After the receive on process $1$ is completed, the send at line \texttt{12} is able to be issued. Finally, the message ``\texttt{1}" is received by process $0$ in variable $B$. The control flow that ``\texttt{B > 0}" is preserved from the original execution and the assert at line \texttt{05} does not fail. As shown in the trace, each message is immediately received by a matching receive. This communication topology is enforced by zero buffer semantics, but is not required by infinite buffer semantics.

%It is possible to execute the same program with another feasible trace (\figref{fig:trace2}). This trace assumes an implementation of infinite buffer semantics. Unlike the first trace, the trace in \figref{fig:trace2} buffers the message ``4" instead of transferring it immediately. The message ``\texttt{Go}" is then sent from process $2$ to process $1$ in variable $C$. After that, the message ``1" on process $1$ is sent out. At this point, the arrival of message ``4" and ``1" are racing and the message ``1" arrives first. As a result, the assume at line \texttt{04} is satisfied but the assertion at line \texttt{05} fails.

%Message non-determinism is significant and difficult to analyze, especially when infinite buffer semantics are enforced which have more possibilities for message communication. Observe that the second trace is only allowed by zero buffer semantics. The message ``\texttt{4}" must be received immediately once it is sent out. As such, there is no way to match the receive at line \texttt{00} and the send at line \texttt{12}.  The assertion at line \texttt{05} does not fail under zero buffer semantics. 


%\subsection{Collective Communication}
%\begin{figure}[c]
%\begin{center}
%\setlength{\tabcolsep}{2pt}
%\small \begin{tabular}[t]{l}
%20 $\mathtt{S(to\ P0, "4", \&h5)}$ \\
%21 $\mathtt{W(\&h5)}$\\
%22 $\mathtt{\underline{B(comm)}}$\\
%\hline
%10 $\mathtt{\underline{B(comm)}}$\\
%\hline
%00 $\mathtt{R(from\ P2, A, \&h1)}$ \\
%01 $\mathtt{W(\&h1)}$ \\
%02 $\mathtt{\underline{B(comm)}}$\\
%\hline
%23 $\mathtt{S(to\ P1, "Go", \&h6)}$ \\
%24 $\mathtt{W(\&h6)}$ \\
%\hline
%11 $\mathtt{R(from\ P2, C, \&h3)}$ \\
%12 $\mathtt{W(\&h3)}$ \\
%13 $\mathtt{S(to\ P0, "1", \&h4)}$ \\
%14 $\mathtt{W(\&h4)}$ \\
%\hline
%03 $\mathtt{R(from\ P1, B, \&h2)}$ \\
%04 $\mathtt{W(\&h2)}$ \\
%05 $\mathtt{assert(A == 4);}$ \\
%\hline
%\end{tabular}
%\end{center}
%\caption{A feasible execution trace of the MPI program execution in \figref{fig:mpi_barrier}}
%\label{fig:trace3}
%\end{figure}

%\examplefigoneB

The analysis of MPI semantics is even harder with collective operations because message communication can be affected. There are various types of collective operations such as barriers and broadcasts, etc., that synchronize an MPI program in the following way: each type of operation is used as a group; each operation in the group blocks the execution of a process until all the operations are completed. We take barrier as an example in the following discussion. We revise the sample program in \figref{fig:mpi} by inserting a group of barriers: a barrier sitting on process \texttt{P0} immediately preceding line $l_3$, a barrier sitting on process \texttt{P1} immediately preceding line $l_9$ and a barrier sitting on process \texttt{P1} immediately preceding line $l_{10}$. Assuming a infinite buffer configuration is chosen, each process waits on a barrier until all the barriers are matched. Therefore, message ``$2$" has no way to be received in variable $A$. Consequently, the assertion on process \texttt{P0} does not fail. The message communication is limited by the barriers. 
%The following sections present an algorithm that encodes the semantics of both point-to-point communication and collective communication into an SMT problem. The encoding can be solved by an SMT solver such as Yices \cite{dutertre:CAV06} and Z3 \cite{demoura:tacas08}, etc.





