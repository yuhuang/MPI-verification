%\section{Collective Communication}
As discussed earlier, collective operations are used to synchronize an MPI program. To be precise, collective operations such as barriers block the execution of processes until all the members in a group are matched. In addition, some type of collective operations such as broadcast are able to send internal messages amongst its group, and/or to execute global operations. MPI semantics guarantee that messages generated on behalf of collective operations are not confused with messages generated by point-to-point operations. Therefore, the encoding in this paper puts emphasis on how to reason about the synchronization of collective operations as it affects point-to-point communication. The internal message passing and the execution of global operations by collective communication can be added as SMT constraints to the encoding. In the following discussion, we take barrier as an example. The barrier is defined as a group in \defref{def:barrier}. 

\begin{definition}[Barrier]\label{def:barrier}
The occurrence of a group of barriers, $\mathtt{B}$ = $\{\mathtt{B_0},$ $\mathtt{B_1},$ $...,$ $\mathtt{B_n}\}$, is captured by a
single timestamp, $\mathit{time}_\mathtt{B}$, that marks when all the members in the group are matched.  
\end{definition}

\begin{figure}[b]
\[
\begin{array}{l|l}
\;\;\;\;\;\;\;\;\mathtt{Process\ 0}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{Process\ 1}\;\;\;\;\;\;\;\; \\
\hline
\;\;\;\;\;\;\;\;\mathtt{\underline{B(comm)}}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{R(from\ P0,A,\&h2)}\;\;\;\;\;\;\;\; \\
\;\;\;\;\;\;\;\;\mathtt{S(to\ P1,``1",\&h1)}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{\underline{B(comm)}}\;\;\;\;\;\;\;\; \\
\;\;\;\;\;\;\;\;\mathtt{W(\&h1)}\;\;\;\;\;\;\;\; & \;\;\;\;\;\;\;\; \mathtt{W(\&h2)}\;\;\;\;\;\;\;\; \\
\end{array}
\]
\caption{Message Communication with Barriers} \label{fig:mc_barrier1}
\end{figure}

Even though barriers affect the issuing order of two events, it is hard to determine whether they prevent a send from matching a receive. As an example, the message ``$1$" in \figref{fig:mc_barrier1} may flow into $\mathtt{R}$ even though $\mathtt{R}$ is ordered before the barrier and $\mathtt{S}$ is ordered after the barrier. The wait $\mathtt{W(\&h2)}$ determines the behavior. If the program had issued $\mathtt{W(\&h2)}$ before the barrier, $\mathtt{R}$ would have to be completed before the barrier, meaning the message is delivered. The encoding further defines the nearest-enclosing barrier (\defref{def:nb}) for this type of interaction.

\begin{definition}[Nearest-Enclosing Barrier]\label{def:nb}
For any process $\mathtt{i}$, a receive $\mathtt{R}$ has a nearest-enclosing barrier $\mathtt{B}$ if and only if
\begin{compactenum}
\item the nearest-enclosing wait, $\mathtt{W}$, of $\mathtt{R}$ is ordered before $\mathtt{B_i}\in \mathtt{B}$, and
\item there does not exist any receive $\mathtt{R^\prime}$ on process $\mathtt{i}$, with a nearest-enclosing wait, $\mathtt{W^\prime}$, such that 1) $\mathtt{W^\prime}$ is ordered after $\mathtt{W}$; and 2) $\mathtt{W^\prime}$ is ordered before $\mathtt{B_i}$.
\end{compactenum}
\end{definition}

Based on the definitions above, the encoding defines two rules for program order in \figref{fig:cc}. Rule $(9)$ only constrains the program order over the nearest-enclosing wait and the nearest-enclosing barrier for a receive. The order over this receive and the nearest-enclosing barrier is not constrained. For rule $(10)$, a barrier has to happen before any operation ordered after it. 

%Given the encoding rules for MPI non-deterministic point-to-point communication, MPI deterministic point-to-point communication and MPI collective communication, we are able encode the sample program in \figref{fig:mpi} under infinite buffer semantics. \figref{fig:mpiencoding}(a) is the encoding. Resolving these formulas by a common SMT solver (e.g., Z3 \cite{demoura:tacas08}) produces a violating trace (e.g., trace $\pi_1$ mentioned earlier). 

%Intuitively, the program order, match pairs and program properties are encoded with respect to infinite buffer semantics. The encoding does not add too many constraints for program order because infinite buffer semantics give more possibilities for message communication. The much pair encoding further gives all possible choices for message communication. The program properties are also encoded so the solver is capable of finding assertion violation for the program.

\encodingcc

%\exampleencoding
