\section{Introduction}
Nowadays, message passing technology has become widely used in many fields such as medical devices and automobile systems. Message Passing Interface (MPI) plays an significant role as a common standard. It is easy for a developer to implement a message passing scenario using MPI point-to-point operations, e.g., send and receive. Along with such convenience, the MPI semantics allow the messages to be received in a wildcard way meaning they may arrive non-deterministically. Such non-determinism often leads to intermittent and unexpected message race and is difficult for a developer to test and debug without exploring the full program state space. During a message race, an accidental message may be received leading to incorrect computation and violation of user provided assertions. 

%: collective operation
Collective operations such as barriers and broadcasts may also change message communication. The use of each category of collective operations synchronize the program in such a way that each process waits on a particular location until all the operations are completed. Given this semantic, a message may not arrive as expected if the runtime order over the send and the receive is ``interrupted" by the collective operations.

%: infinite buffer vs. zero buffer
The message race is also affected by two runtime environments: infinite-buffering (the message is copied into a runtime buffer on the API call) or zero-buffering (the message has no buffering) \cite{DBLP:conf/fm/VakkalankaVGK09}. These two semantics may treat the message communication very differently. Under infinite buffer semantics, a message non-deterministically choose to be temporarily buffered or to be immediately received once it is sent out. As such, more resolutions on matching a send operation and a receive operation for this message may exist in runtime. In contrast, zero buffer semantics do not allow a message to be buffered in any process. A process waits without proceeding until the pending message is received. Consequently, zero buffer semantics have less choices to resolve the send-receive matches. 

%zero buffer compatibility
Because of this inflexible behavior, many MPI applications are not compatible with zero buffer semantics. It is difficult for a developer to notice this incompatibility. As such, an execution without the pre-knowledge may cause disastrous results such as deadlock. 
%Therefore, the freedom of message passing can be balanced out in some way with respect to the runtime behaviors. 

%related works
Based the discussion above, the message race and the zero buffer compatibility can be much difficult to analyze. Several solutions are provided in this field. The POE algorithm is capable of dynamically analyzing the behavior of an MPI program \cite{DBLP:conf/ppopp/VakkalankaSGK08}. By postponing the cooperative operations for message passing in transit, it is possible to reduce the program state space. While it is correct in detecting errors caused by unexpected message race, it suffers from the scalability issue thus is hard to run to completion if the number of messages grows increasingly. 

Vo. et al. develop a dynamic analysis tool for large-scale MPI programs using lamport clocks with lazy updates \cite{DBLP:conf/sc/VoAGSSB10, DBLP:conf/IEEEpact/VoGKSSB11}. This method detects the potential send-receive matches by collecting the piggyback data, however, is unable to obtain the full set of matches. As a result, this method does not provide completeness meaning all possible errors can be detected, even though it scales well practically.

In the context of Multicore Application Interface (MCAPI), another message passing API standard, Sharma et al. propose the first push button model checker -- MCC \cite{DBLP:conf/fmcad/SharmaGMH09}. It indirectly controls the MCAPI runtime to verify MCAPI programs under zero buffer semantics. An obvious drawback of this work is its inability to analyze infinite buffer semantics which is known as a common runtime environment in message passing. A key insight, though, is the direct use of match pairs --couplings for potential sends and receives.

Elwakil et al. encode a message passing program execution into a Satisfiability Modulo Theories (SMT) \cite{barrett2008satisfiability} problem \cite{DBLP:conf/issta/ElwakilY10}. This approach fails for two reasons. First, the encoding does not support infinite-buffering. Second, it assumes that a precise set of potential send-receive matches is given as an input to the encoding. However, to generate such a set is non-trivial.

%our solution: SMT, statically encode MPI semantics
This paper extends the prior work that encodes an MCAPI (the Multicore Communications API \cite{mcapi}) execution into an SMT problem \cite{DBLP:conf/kbse/HuangMM13}. To support MPI semantics, this paper presents a new algorithm that takes input an MPI execution trace and a set of over-approximated send-receive matches. The trace is generated under infinite buffer semantics. This algorithm encodes the point-to-point operations and the collective operations as SMT constraints. By resolving these constraints, an SMT solver is able to report an assertion violation if there is a satisfying assignment; or to prove the execution is correct if it is unsatisfiable. The encoding supports both infinite buffer semantics and zero buffer semantics with respective rules. To summarize, the main contributions of this paper include,
%Contribution:
\begin{itemize}
\item a new encoding algorithm that supports MPI point-to-point communication and MPI collective communication,
\item a dramatic improved zero-buffer encoding that is capable of detecting the zero-buffer compatibility, and
\item a set of benchmarks that demonstrates the correctness and efficiency of the new encoding.
\end{itemize}

%section Organization
The rest of the paper is organized as follows: section $2$ discusses the MPI semantics given two examples; section $3$ gives the encoding algorithm; section $4$ discusses the zero-buffer compatibility; section $5$ gives the experiment results; section $6$ discusses the related works; and section $7$ discusses the conclusions and future work.

\examplefigone


           