\section{Introduction}
Nowadays, message passing technology has become widely used in many fields such as medical devices and automobile systems. Message Passing Interface (MPI) plays an significant role as a common standard. It is easy for a developer to implement a message passing scenario using MPI point-to-point operations, i.e., send and receive, and collective operations, i.e., barrier. Along with such convenience, the MPI semantics allow the messages can be received in a wildcard way meaning they can arrive non-deterministically. Such non-determinism often leads to intermittent and unexpected message race and is difficult for a developer to test and debug without exploring the full program state space. During a message race, an accidental message may be received leading to incorrect computation and violation of user provided assertions. 

%: infinite buffer vs. zero buffer
The message race is also affected by two runtime environments: infinite-buffering (the message is copied into a runtime buffer on the API call) or zero-buffering (the message has no buffering) \cite{}. These two semantics may treat the message passing very differently. Under a infinite buffer setting, the message has a choice to be buffered or to be delivered with respect to the runtime behavior. Because of the flexibility, more runtime behaviors can be included in an MPI execution. In contrast, zero-buffering does not allow a message to be buffered. In other words, the program waits until the pending message is delivered. Consequently, zero-buffering has less runtime behaviors.

%: collective operation
Collective operations such as barriers may also change the message communication. The collective operations synchronize the program in such a way that each process waits on a particular location until a set of collective operations are matched in a group. Given this semantic, a message may be ``blocked" from arriving at a specific receive operation because of the collective operations. 

%Therefore, the freedom of message passing can be balanced out in some way with respect to the runtime behaviors. 

%related works
From the discussion above, the message race can be much difficult to analyze. Several solutions are provided in this field. The POE algorithm is capable of dynamically predicting the runtime behaviors in an MPI program execution \cite{}. It is a DPOR (Dynamic Partial Order Reduction) algorithm applied to MPI programs \cite{}. It operates by postponing the cooperative operations for message passing in transit until each process reaches a blocking call. It then determines the potential matches of send and receive operations in runtime. While it is efficient in detecting errors caused by unexpected message race, it suffers from the scalability issue thus is hard to run to completion if the number of messages grows increasingly. 

Vo. et al. develop a dynamic analysis tool for large-scale MPI programs using lamport clocks with lazy updates \cite{}. The auxiliary information by setting up a logic clock is sent out and updated via piggyback messages. This method detects the potential send-receive matches by collecting the piggyback data, however, is unable to obtain the full set of matches. As a result, this method does not provide completeness meaning all possible errors can be detected, even though it scales well practically.

In the context of Multicore Application Interface (MCAPI), another message passing API standard, Sharma et al. propose the first push button model checker -- MCC \cite{}. It indirectly controls the MCAPI runtime to verify MCAPI programs under zero-buffer semantics. An obvious drawback of this work is its inability to analyze infinite-buffering which is known as a common runtime environment in message passing. The work uses a dynamic partial order reduction for the model checker, but is still insufficient to avoid the state space explosion in the presence of even moderate non-determinism between message sends and receives. A key insight, though, is the direct use of match pairs --couplings for potential sends and receives.

Elwakil et al. encode a message passing program execution into a Satisfiability Modulo Theories (SMT) problem \cite{}. This approach fails for two reasons. First, the encoding dos not support infinite-buffering. Second, it assumes that a precise set of potential send-receive matches is given as an input to the encoding. However, to generate such a set is non-trivial.

%our solution: SMT, statically encode MPI semantics
This paper presents a encoding algorithm that takes input an MPI execution trace and a set of over-approximated send-receive matches. This algorithm encodes non-blocking send and receive as point-to-point operations and barrier as collective operation into constraints. By resolving these constraints, an SMT solver is able to report an assertion violation if there is a satisfying assignment; or to prove the execution is correct if it is unsatisfying.  The encoding supports both infinite-buffering and zero-buffering semantics with different rules. The encoding requires fewer terms to capture all possible program behavior when compared to other proposed methods. To summarize, the main contributions of this paper is,
%Contribution:
\begin{itemize}
\item a correct and efficient SMT encoding algorithm to define the MPI behaviors under both infinite-buffering and zero-buffering so that assertion violations in an MPI program execution can be detected
\end{itemize}

%section Organization
The rest of the paper is organized as follows: section $2$ discusses the MPI semantics given two examples; section $3$ gives the encoding algorithm; section 4 gives the experiment results; section 5 discusses the related works; and section 6 discusses the conclusions and future work.

\examplefigone


           