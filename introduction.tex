\section{Introduction}
Nowadays, message passing technology has become widely used in many fields such as medical devices and automobile systems. Message Passing Interface (MPI) plays an significant role as a common standard. It is easy for a developer to implement a message passing scenario using MPI point-to-point operations, i.e., send and receive, and collective operations, i.e., barrier. Along with such convenience, the MPI semantics provides more freedom of message passing. As a result, the messages can arrive non-deterministically depending on the runtime behaviors. Such non-determinism often leads to intermittent and unexpected message race and is difficult for a developer to test and debug without exploring the entire state space of the program. In a message race, two or more messages may arrive at an identical process simultaneously. The variable sitting in a receive operation may obtain a unexpected message that leads to a incorrect computation. If the correctness property is checked in a user provided assertion, a failure should be returned. 

%: infinite buffer vs. zero buffer
The message race can also be affected by two runtime environments: infinite-buffering (the message is copied into a runtime buffer on the API call) or zero-buffering (the message has no buffering) \cite{}. These two semantics may treat the message passing very differently. Under a infinite buffer setting, the message has a choice to be buffered or to be delivered with respect to the runtime behavior. Because of the flexibility, more runtime behaviors can be included in an MPI execution. In contrast, zero-buffering does not allow a message to be buffered. In other words, the program waits until the pending message is delivered. Consequently, zero-buffering has less runtime behaviors.

%: collective operation
Collective operations such as barriers may also change the way a message arrives. The collective operations synchronize the program in such a way that each process waits on a particular location until a set of collective operations are matched in a group. Given this semantics, a message arriving may be ``blocked" because it is unable to be sent out. Therefore, the freedom of message passing can be balanced out in some way with respect to the runtime behaviors. 

%related works
From the discussion above, the message race can be much difficult to analyze. Several solutions are provided in this field. The POE algorithm is capable of dynamically predicting the runtime behaviors in an MPI program execution \cite{}. It is a DPOR (Dynamic Partial Order Reduction) algorithm applied to MPI programs \cite{}. It operates by postponing the cooperative operations for message passing in transit until each process reaches a blocking call. It then determines the potential matches of send and receive operations in runtime. While it is efficient in detecting errors caused by unexpected message race, it suffers from the scalability issue thus is hard to run to completion if the number of messages grows increasingly. 

Vo. et al. develop a dynamic analysis tool for large-scale MPI programs using lamport clocks with lazy updates \cite{}. The auxiliary information by setting up a logic clock is sent out and updated via piggyback messages. This method detects the potential send-receive matches by collecting the piggyback data, however, is unable to obtain the full set of matches. As a result, this method does not provide completeness meaning all possible errors can be detected, even though it scales well practically.

In the context of Multicore Application Interface (MCAPI), another message passing API standard, Sharma et al. propose the first push button model checker -- MCC \cite{}. It indirectly controls the MCAPI runtime to verify MCAPI programs under zero-buffer semantics. An obvious drawback of this work is its inability to analyze infinite-buffering which is known as a common runtime environment in message passing. The work uses a dynamic partial order reduction for the model checker, but is still insufficient to avoid the state space explosion in the presence of even moderate non-determinism between message sends and receives. A key insight, though, is the direct use of match pairs --couplings for potential sends and receives.

Elwakil et al. encode a message passing program execution into a Satisfiability Modulo Theories (SMT) problem \cite{}. This approach fails for two reasons. First, the encoding dos not support infinite-buffering. Second, it assumes that a precise set of potential send-receive matches is given as an input to the encoding. However, to generate such a set is non-trivial.

%our solution: SMT, statically encode MPI semantics
This paper presents a encoding algorithm that takes input an MPI execution trace and a set of over-approximated send-receive matches (generated by a polynomial algorithm discussed later). This encoding defines both point-to-point and collective operation semantics into constraints. By resolving these constraints, an SMT solver is able to report an assertion violation if there is a satisfying assignment; or to prove the execution is correct if it is unsatisfying.  The encoding rules can also be adjusted to either infinite-buffering or zero-buffering semantics by adding and/or removing some constraints. The encoding requires fewer terms to capture all possible program behavior when compared to other proposed methods making it more efficient in the SMT solver. To obtain the correct behavior of message passing, the paper further proposes an algorithm that generates an over-approximation of potential send-receive matches with low cost. The matches generated, even though not precise, is able to provide sufficient information that can be used by an SMT solver for message non-determinism. To summarize, the main contributions of this paper include,
%Contribution:
\begin{compactenum}
\item a correct and efficient SMT encoding algorithm to define the MPI behaviors under both infinite-buffering and zero-buffering so that assertion violations in an MPI program execution can be detected;
\item an O($N^2$) algorithm to generate an over-approximation of possible send-receive matches in an MPI program execution, where $N$ is the size of the execution trace in lines of code. 
\end{compactenum}

%section Organization
The rest of the paper is organized as follows: section $2$ discusses the MPI semantics given two examples; section $3$ gives the encoding algorithm; section $4$ gives the algorithm to generate an over-approximation of potential send-receive matches; section 5 gives the experiment results; section 6 discusses the related works; and section 7 discusses the conclusions and future work.

\examplefigone


           