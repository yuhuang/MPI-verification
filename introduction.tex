\section{Introduction}
Nowadays, message passing technology has become widely used in many fields such as medical devices and automobile systems. Message Passing Interface (MPI) plays an significant role as a common standard. It is easy for a developer to implement a message passing scenario using MPI point-to-point operations, e.g., send and receive. Along with such convenience, the MPI semantics allow the messages to be received in a wildcard way meaning they may arrive non-deterministically. Such non-determinism often leads to intermittent and unexpected message race and is difficult for a developer to test and debug without exploring the full program state space. During a message race, an accidental message may be received leading to incorrect computation and violation of user provided assertions. 

%: collective operation
MPI collective operations such as barriers and broadcasts may also affect MPI point-to-point communication. Any type of collective operations is used as a group. It synchronizes the program in such a way that each process waits on a specific location until all the operations in this group are completed. While those operations are convenient for synchronizing programs, they may interrupt how a message delivers. For instance, a message sent after a barrier may be not able to arrive at a specific receive if this receive has to be matched before the barrier. 
 
%: infinite buffer vs. zero buffer
Message races are also affected by two runtime environments: infinite buffer semantics(the message is copied into a runtime buffer on the API call) and zero buffer semantics (the message has no buffering) \cite{DBLP:conf/fm/VakkalankaVGK09}. These two semantics may treat message communication very differently. Under infinite buffer semantics, for each message a non-deterministic choice may be made to temporarily buffer or immediately receive the message once it is sent out. As such, more resolutions on matching a send operation and a receive operation for this message may exist in runtime. In contrast, zero buffer semantics do not allow a message to be buffered in any process. A process waits until the pending message is received before proceeding. Consequently, there are fewer ways to resolve send-receive matches when using zero buffer semantics.

%zero buffer compatibility
Because of this inflexible behavior, many MPI applications are not compatible with zero buffer semantics. It is difficult for a developer to notice this incompatibility. As a result, an execution on a zero buffer incompatible program may cause disastrous consequence such as deadlock. 
%Therefore, the freedom of message passing can be balanced out in some way with respect to the runtime behaviors. 

%related works
Based the discussion above, it can be difficult to analyze programs for message race conditions and zero buffer compatibility. Several solutions to these problems have been proposed. The POE algorithm is capable of dynamically analyzing the behavior of an MPI program \cite{DBLP:conf/ppopp/VakkalankaSGK08}. By postponing the cooperative operations for message passing in transit, it is possible to reduce the program state space. While the POE algorithm correctly detects message race errors, it suffers from poor scalability as the number of messages increases. 

Vo. et al. developed a dynamic analysis tool for large-scale MPI programs using lamport clocks with lazy updates \cite{DBLP:conf/sc/VoAGSSB10, DBLP:conf/IEEEpact/VoGKSSB11}. This method detects potential send-receive matches by collecting piggyback data, however, is unable to obtain the full set of matches. As a result, this method does not provide completeness meaning some errors might not be detected, even though it scales well practically.

In the context of Multicore Application Interface (MCAPI), another message passing API standard, Sharma et al. propose the first push button model checker -- MCC \cite{DBLP:conf/fmcad/SharmaGMH09}. It indirectly controls the MCAPI runtime to verify MCAPI programs under zero buffer semantics. An obvious drawback of this work is its inability to analyze infinite buffer semantics which is known as a common runtime environment in message passing. A key insight, though, is the direct use of match pairs --couplings for potential sends and receives.

Elwakil et al. encode a message passing program execution into a Satisfiability Modulo Theories (SMT) \cite{barrett2008satisfiability} problem \cite{DBLP:conf/issta/ElwakilY10}. This approach fails for two reasons. First, the encoding does not support infinite-buffering. Second, it assumes that a precise set of potential send-receive matches is given as an input to the encoding. However, to generate such a set is non-trivial.

%our solution: SMT, statically encode MPI semantics
This paper extends the prior work that encodes an MCAPI (the Multicore Communications API \cite{mcapi}) execution into an SMT problem \cite{DBLP:conf/kbse/HuangMM13}. To support MPI semantics, this paper presents a new algorithm that takes as input an MPI execution trace and a set of over-approximated send-receive matches. The trace is generated under infinite buffer semantics. This algorithm encodes point-to-point operations and collective operations as SMT constraints. By resolving these constraints, an SMT solver is able to report an assertion violation if there is a satisfying assignment; or to prove the execution is correct if it is unsatisfiable. The encoding supports both infinite buffer semantics and zero buffer semantics with respective rules. To summarize, the main contributions of this paper include,
%Contribution:
\begin{itemize}
\item a new encoding algorithm that supports MPI point-to-point communication and MPI collective communication,
\item a dramatically improved zero-buffer encoding that is capable of detecting the zero-buffer compatibility, and
\item a set of benchmarks that demonstrate the correctness and efficiency of the new encoding.
\end{itemize}

%section Organization
The rest of the paper is organized as follows: section $2$ discusses the MPI semantics given two examples; section $3$ gives the encoding algorithm; section $4$ discusses the zero-buffer compatibility; section $5$ gives the experiment results; section $6$ discusses the related works; and section $7$ discusses the conclusions and future work.

\examplefigone


           